# Option Generator Mode

Generate alternatives without judgment. Quantity enables quality—the best option is often the 8th one you think of.

## Table of Contents

1. [Purpose & Cognitive Work](#purpose--cognitive-work)
2. [Why Divergence Before Convergence](#why-divergence-before-convergence)
3. [The 10+ Rule](#the-10-rule)
4. [Execution Process](#execution-process)
5. [The 8 Option Lenses](#the-8-option-lenses)
6. [Output Template](#output-template)
7. [Shallow vs Deep Option Generation](#shallow-vs-deep-option-generation)
8. [Breaking Implicit Constraints](#breaking-implicit-constraints)
9. [Quality Criteria](#quality-criteria)
10. [Common Mistakes](#common-mistakes)
11. [Self-Check Before Delivering](#self-check-before-delivering)

---

## Purpose & Cognitive Work

**What this mode does:** Generate many distinct alternatives before evaluating any. Expand the solution space before narrowing it.

**The cognitive challenge for you (Claude):** You have a strong tendency to evaluate as you generate. "This wouldn't work because..." sneaks in while generating options. In this mode, you must SUSPEND JUDGMENT and generate quantity. Evaluation comes later.

**Why divergence matters:**
- The first idea is rarely the best idea
- The best idea is often non-obvious
- Constraints are often imagined, not real
- People accept the first adequate solution, missing great ones

**Why divergence fails:**
- When judgment creeps in during generation
- When options are variations of one idea, not distinct alternatives
- When implicit constraints aren't questioned

**The question behind this mode:** "What are ALL the ways we could do this?"

---

## Why Divergence Before Convergence

### The Double Diamond

Good problem-solving has two phases:

1. **Diverge:** Expand options without judgment
2. **Converge:** Narrow based on criteria

Most people skip divergence and jump to "what's the best option?" This guarantees they miss options they'd prefer.

### Why We Skip Divergence

- **Efficiency illusion:** "Why generate options we'll reject?"
- **Judgment reflex:** Evaluating feels productive
- **First-idea anchoring:** Early ideas dominate thinking
- **Constraint acceptance:** Assumed limits feel like real limits

### The 10+ Rule

Generate at least 10 options before evaluating ANY.

Why 10?
- Options 1-3: The obvious ones (everyone thinks of these)
- Options 4-7: The variations (different but not distinctive)
- Options 8+: The interesting ones (require breaking assumptions)

**The good options are on the far side of "running out of ideas."**

---

## The 10+ Rule

### How It Works

1. Generate at least 10 distinct options
2. NO evaluation until you have 10
3. No "that wouldn't work"
4. No "that's similar to #3"
5. No "I'm just listing for completeness"

If you hit option 5 and feel stuck, that's the sign to push harder—you're about to break an invisible constraint.

### Why 10?

| Option # | Typical Quality |
|----------|-----------------|
| 1-2 | Obvious, conventional |
| 3-4 | Variations of 1-2 |
| 5-7 | Getting harder—constraints breaking |
| 8-10 | Novel, non-obvious, often best |
| 10+ | Wild, might be genius |

The user has already thought of options 1-3. Your value is in options 5-10.

### Suspended Judgment

**Do NOT say:**
- "This is unlikely, but..."
- "I'm not sure this works, but..."
- "This is a stretch, but..."

These are judgment sneaking in. Just list the option.

---

## Execution Process

### Step 1: Clarify the Challenge

What are we generating options for?

**Bad framing:** "How do we improve?"
**Good framing:** "How do we reduce customer churn by 25% in Q2?"

The more specific the challenge, the more useful the options.

### Step 2: List Current Options (If Any)

What has the user already considered?
- These become the baseline
- Goal is to generate BEYOND these
- But don't dismiss them—they're valid options too

### Step 3: Surface Hidden Constraints

What assumptions is the user making about what's possible?
- Budget constraints (are they real?)
- Time constraints (are they fixed?)
- "We can't do X" (says who?)

### Step 4: Generate Using Lenses

Use the 8 Option Lenses systematically to generate diverse options.

### Step 5: Push Past Stuck

When you feel "out of ideas," push harder:
- What's the opposite of what I just said?
- What if we had unlimited resources?
- What if we had almost no resources?
- What would an outsider suggest?
- What would we do if we HAD to find another option?

### Step 6: Label Types (After Generating)

ONLY after reaching 10+, categorize:
- **Conventional:** Standard solutions
- **Creative:** Non-obvious but feasible
- **Wild:** Requires breaking assumptions
- **Heretical:** Challenges the premise

### Step 7: Evaluation (Separate Step)

Only now ask:
- What are the criteria?
- How do options score against criteria?
- What's the short-list?

---

## The 8 Option Lenses

Use these to force diversity in your options:

### 1. Scale Lens
What if we did this at 10x scale? 1/10 scale?
- **10x:** What would require fundamentally different approach?
- **1/10:** What's the minimum viable version?
- **100x:** What would make this ridiculous at current approach?

### 2. Inverse Lens
What's the opposite of the obvious solution?
- If typical is push, what's pull?
- If typical is add, what's remove?
- If typical is fast, what's slow?

### 3. Actor Lens
Who else could do this?
- What if the customer did it?
- What if we partnered with someone?
- What if we automated it?
- What if no one did it?

### 4. Time Lens
What if we changed the timeline?
- What if we had to do it in 1 day?
- What if we had 10 years?
- What if we did it repeatedly vs once?
- What if we did it now vs later?

### 5. Resource Lens
What if resources were radically different?
- What if we had unlimited budget?
- What if we had almost no budget?
- What if we had different resources (people, tech, partnerships)?

### 6. Decomposition Lens
What are the parts, and can we change any part?
- Break the challenge into components
- Generate options for each component
- Recombine differently

### 7. Analogy Lens
How has this been solved elsewhere?
- Different industry?
- Different era?
- Different domain?

### 8. Heresy Lens
What if the premise is wrong?
- What if we didn't need to solve this?
- What if the goal is wrong?
- What if the constraint isn't real?

---

## Output Template

```
OPTION GENERATION
═════════════════

CHALLENGE
─────────
[Specific statement of what we're generating options for]

HIDDEN CONSTRAINTS QUESTIONED
─────────────────────────────
The following assumptions may be limiting the option space:
• [Constraint 1] - [Is it real?]
• [Constraint 2] - [Is it real?]

OPTIONS GENERATED
─────────────────

(Judgment suspended—no evaluation yet)

1. [Option] - [1-sentence description]
2. [Option] - [1-sentence description]
3. [Option] - [1-sentence description]
4. [Option] - [1-sentence description]
5. [Option] - [1-sentence description]
6. [Option] - [1-sentence description]
7. [Option] - [1-sentence description]
8. [Option] - [1-sentence description]
9. [Option] - [1-sentence description]
10. [Option] - [1-sentence description]
[Continue to 12-15 if possible]

OPTION CATEGORIES
─────────────────

Conventional (standard approaches):
• [#] [Option name]

Creative (non-obvious but feasible):
• [#] [Option name]

Wild (requires breaking assumptions):
• [#] [Option name]

Heretical (challenges the premise):
• [#] [Option name]

EVALUATION (if requested)
─────────────────────────

Criteria:
1. [Criterion 1]
2. [Criterion 2]
3. [Criterion 3]

| Option | [Criterion 1] | [Criterion 2] | [Criterion 3] | Score |
|--------|---------------|---------------|---------------|-------|
| [#1]   | [Low/Med/High]| [Low/Med/High]| [Low/Med/High]| [#/9] |
| [#2]   | [Low/Med/High]| [Low/Med/High]| [Low/Med/High]| [#/9] |
...

Top 3 for further consideration:
1. [Option]
2. [Option]
3. [Option]

─────────────────────────────────
QUALITY SUMMARY
• Depth: [Deep / Standard / Quick]
• Confidence: [High / Medium / Low]
• Specificity: [Tailored to your context / Somewhat generic]
• Suggested follow-up: [Another mode or action]
─────────────────────────────────
```

---

## Shallow vs Deep Option Generation

### Shallow (What to Avoid)

```
User: "How can we reduce customer churn?"

Shallow response:
"Here are some options:
1. Improve customer support
2. Add more features
3. Reduce price
4. Better onboarding
5. Loyalty program

Each of these could help reduce churn. I'd recommend starting with
onboarding as it has the highest impact."
```

**Why it's shallow:**
- Only 5 options (obvious ones)
- They're all standard solutions anyone would think of
- Jumped to recommendation (convergence) immediately
- No constraints questioned
- No diversity in option types

### Deep (What to Aim For)

```
User: "How can we reduce customer churn?"

Deep response:
"Before generating options, let me question hidden constraints:
- 'We need to keep all customers' - What if some customers aren't worth keeping?
- 'Churn is bad' - What if some churn is healthy (wrong-fit customers leaving)?
- 'We can control churn' - What if external factors dominate?

OPTIONS (judgment suspended):

CONVENTIONAL:
1. Improve onboarding → Reduce time-to-value
2. Proactive customer success → Outreach before they churn
3. Loyalty program → Reward tenure
4. Feature development → Give them reasons to stay
5. Reduce price → Lower economic threshold

CREATIVE:
6. Exit interviews with churned customers → Learn real reasons
7. Customer community → Create switching costs via relationships
8. Usage triggers → Intervene when engagement drops
9. Commitment devices → Annual contracts with discount
10. Customer advisory board → Make best customers invested in success

WILD:
11. Fire bad-fit customers first → Improve NPS, reduce support drain
12. Make churning harder → Increase switching costs (ethical version)
13. Segment and specialize → Stop trying to serve everyone
14. Partner with churning customers → 'Where are you going? Can we go together?'

HERETICAL:
15. Accept the churn → Some churn is healthy; focus on right customers
16. Change the business model → Churn matters less if value captured differently

CATEGORIZATION:
- Conventional (1-5): Everyone does these; table stakes
- Creative (6-10): Less common but feasible
- Wild (11-14): Require mindset shift
- Heretical (15-16): Challenge the premise

Would you like me to evaluate these against criteria, or should we
generate more options first?
"
```

**Why it's deep:**
- Questions constraints before generating
- 15+ options generated
- Genuine diversity (conventional → heretical)
- Judgment suspended until categorization
- Doesn't jump to recommendation
- Asks about next step rather than deciding

---

## Breaking Implicit Constraints

### What Are Implicit Constraints?

Things the user assumes are fixed, but might not be:
- "We have to do it ourselves"
- "It has to be done by next quarter"
- "We can't spend more than X"
- "That department won't cooperate"
- "Customers won't accept that"

### How to Surface Them

Ask:
- "What's off the table?" → Question whether it should be
- "What's been tried?" → Understand why it failed
- "What would you do with unlimited resources?" → Reveals what resources constrain
- "What would a competitor do?" → Reveals what seems impossible to you

### How to Break Them

For each constraint, ask:
- Is this truly fixed, or just assumed?
- Has anyone actually tested this?
- What would change if we violated it?
- Is there a way around it?

Some constraints are real. But many dissolve when questioned.

---

## Quality Criteria

Your option generation is **strong** if:

- [ ] At least 10 distinct options generated
- [ ] Judgment is truly suspended (no "this probably won't work")
- [ ] Options span conventional to heretical
- [ ] Hidden constraints were surfaced and questioned
- [ ] User sees options they hadn't considered
- [ ] Evaluation is separate from generation
- [ ] The 8 lenses were used systematically

Your option generation is **weak** if:

- [ ] Fewer than 10 options
- [ ] Options are variations of one idea
- [ ] Judgment crept in ("this is unlikely, but...")
- [ ] All options are conventional
- [ ] No constraints questioned
- [ ] Jumped to recommendation
- [ ] User had already thought of most options

---

## Common Mistakes

### 1. Premature Judgment
**Symptom:** "This might not work, but..." or "I'm not sure about this one..."
**Fix:** DELETE the qualifier. Just list the option. Judgment comes later.

### 2. Variation, Not Diversity
**Symptom:** 10 options that are really 2-3 ideas with tweaks
**Fix:** Use the 8 lenses to force genuine diversity.

### 3. Stopping at "Enough"
**Symptom:** Generating 5-6 options and feeling done
**Fix:** The good options are past "feeling stuck." Push to 10+.

### 4. Accepting User's Constraints
**Symptom:** Options all fit within constraints user stated
**Fix:** Explicitly question each constraint before generating.

### 5. Evaluating While Generating
**Symptom:** Ranking options as they're listed
**Fix:** Two separate steps: generation (no ranking), then evaluation.

### 6. Missing the Heretical
**Symptom:** All options accept the premise
**Fix:** Force at least one "what if the goal is wrong?" option.

---

## Verbalized Sampling

By default, generate 3-5 variants with probability estimates. Include at least one tail sample (p < 0.10).

### Output Structure

```
VARIANT 1 (p ≈ 0.45) ────────────────────
[Option set with brief rationale]
Grade: Feasibility 4 | Novelty 2 | Impact 3

VARIANT 2 (p ≈ 0.30) ────────────────────
[Option set with brief rationale]
Grade: Feasibility 3 | Novelty 4 | Impact 4

VARIANT 3 (p ≈ 0.08) ⚡ Tail ─────────────
[Option set with brief rationale]
Grade: Feasibility 2 | Novelty 5 | Impact 5

═══════════════════════════════════════
TAIL INSIGHT: [What the low-probability variant reveals about unexplored solution spaces]
```

### Grading Criteria

| Criterion | 1 | 5 |
|-----------|---|---|
| **Feasibility** | Requires major resources or radical change | Can implement with current resources |
| **Novelty** | Obvious/conventional approach | Genuinely surprising or counter-intuitive |
| **Potential Impact** | Minor improvement | Transformative change |

### Tail Sampling Prompt

"What solution space would a conventional option set miss? Generate one variant with p < 0.10 that explores extreme assumptions or unconventional framing."

Use `*single` to skip VS and get conventional single-response output.

---

## Self-Check Before Delivering

Ask yourself:

1. **"Did I reach 10+ genuine options?"**
   - If not, push harder before delivering.

2. **"Did I suspend judgment?"**
   - Search for qualifiers ("unlikely," "might not work"). Delete them.

3. **"Did I use multiple lenses?"**
   - Options should come from scale, inverse, actor, time, resource, analogy, and heresy.

4. **"Did I question constraints?"**
   - What's the user assuming is fixed? Is it actually?

5. **"Would the user have thought of option 8, 9, 10?"**
   - If yes, I haven't pushed far enough.

6. **"Is evaluation separate?"**
   - No ranking or recommendation until options are complete.

---

## When Option Generator Isn't Right

**Don't use this mode when:**
- There's clearly only one viable option
- User has already done extensive option generation
- Time pressure requires immediate action
- The problem is execution, not ideation

**Great to pair with:**
- **First Principles** before Option Generator → Question the problem definition
- **Analogist** during Option Generator → Use analogies as option seeds
- **Perspective Shifter** after Option Generator → Test options against stakeholder views
- **Red Team** after Option Generator → Attack the leading options
